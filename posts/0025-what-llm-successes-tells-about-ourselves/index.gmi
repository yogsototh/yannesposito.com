# What LLM successes tells about ourselves?
description:  And if Human praised mind was in fact not much than
description:  mostly an LLM with a few other layers of features.
keywords:  blog static
author:  Yann Esposito
email:  yann@esposito.host
=> /files/publickey.txt gpg
date:  [2023-05-13 Sat]

I would date the /AI renaissance/ from 2006 when deep-learning was
co-discovered[fn:discovered] by different researchers.
More recently we discovered large language models (LLMs) that are so stunning
this really is the new trend right now.
A lot has been written about it already, but one aspect of the discussion I
find lacking is how this change how we could look at ourselves Human.

If you take the time to read

=> https://arxiv.org/pdf/2303.12712.pdf Sparks of General Artificial Intelligence: Early Experiments

it is clear that AI progress toward something closer and closer to us Humans.
In fact, for some specific cases, the AI has even super-human competences.

So, if LLMs are very good at simulating Human conversation, isn't this because,
our brains are mostly LLM?
LLM are not enough to support all the specter of Human cognition, but are
very good at simulating many aspects of it.

I just like this idea that, if our brain function are close to LLMs,
this give us a lot of opportunities to introspect part of Human behavior.

Note this could also be a dangerous bias to have. As other would consider that
Human are not much than LLM, so studying LLMs would be a very good indicator to
Human behavior.

But we could use this idea to change how we are reflecting on different
subjects.
For example social networks.

If you look at how chatGPT behave in a conversation; it is a lot like us.
We start talking about a subject, and suddenly we kind of gravitate around this subject.
In our day-to-day (I would say "normal mode") we quite often just regurgitate
what we heard using our own words. And this is almost exclusively what LLMs are
made for. Regurgitate what they are trained with.

For the most part of our live, we do not create any real new value, we just
regurgitate the same stories and point of view over and over again.
Once in a while, we are exposed to a new concept, a new experience, and this
could affect our internal database of opinions, point of view, examples,
arguments, etcâ€¦
We add it, and it helps us keep a conversation next time this subject could be
used.

It looks like this could partially explain the efficiency of social networks.
A very big part of our interactions with others is sharing our experience.
But it need a lot more "internal power" to be open to changing our opinion.
Even when we appear to change it, most of the time this is also a social
exchange aspect of a conversation.
Because, we just regurgitated a story for which we do not have strong opinion
about.
And someone else, has a convincing argument about it, and we either really add
this to our system, or just have a generic response.
But the important aspect of it, is that, for the most part of these exchanges,
they are fruitless.
People try to share something, but, most of the time, almost nothing really get
shared to the other minds.


[fn:discovered] /discovered/ is an important nuance as this is a lot more
powerful than "invented". As a good example about why you can see this presentation.

=> /index.gmi Home
=> /gem-atom.xml Feed
=> /slides.gmi Slides
=> /about-me.gmi About

=> https://gitea.esy.fun code
=> https://espial.esy.fun/u:yogsototh bookmarks
=> https://espial.esy.fun/u:yogsototh/notes notes
